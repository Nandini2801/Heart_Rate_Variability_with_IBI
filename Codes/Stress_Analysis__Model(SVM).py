# -*- coding: utf-8 -*-
"""HRV_SVM_TeamB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16tFW-MLYemfMHERVZZ6ifCC196D4roCF
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

trainFile = pd.read_csv("/content/drive/MyDrive/train_hrv.csv")
testFile = pd.read_csv("/content/drive/MyDrive/test_hrv.csv")

complete_data= pd.concat([trainFile,testFile])

complete_data

complete_data.info()

complete_data_new=complete_data[["MEAN_RR", "MEDIAN_RR", "SDRR", "RMSSD", "SDSD", "HR", "pNN25", "pNN50","condition"]]
complete_data_new

complete_data_new.info()

complete_data_new.describe()

!pip install scikit-learn-intelex

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import balanced_accuracy_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils.class_weight import compute_sample_weight

df = complete_data_new
condns = {'no stress': 0,
             'time pressure': 1,
             'interruption': 2}

df['condition'] = df['condition'].map(condns) # re-mapping


X = df.drop('condition', axis=1).copy()
y = df['condition'].copy()

# splitting complete data into " stratified" train and test split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42, stratify=y)

from sklearn.preprocessing import MinMaxScaler
scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)
X_train = scaling.transform(X_train)
X_test = scaling.transform(X_test)

Xvis = X[['MEAN_RR','SDRR']][:1000]
yvis = y[:1000]
Xvis_train, Xvis_test, yvis_train, yvis_test = train_test_split(Xvis, yvis,test_size=0.3, random_state=42, stratify=yvis)

l = SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(Xvis_train, yvis_train)
r = SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(Xvis_train, yvis_train)
p = SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(Xvis_train, yvis_train)
s = SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(Xvis_train, yvis_train)

#stepsize in the mesh, it alters the accuracy of the plotprint
#to better understand it, just play with the value, change it and print it
h = 0.1
Xvis = Xvis.to_numpy()
yvis = yvis.to_numpy()
#create the mesh
x_min, x_max = Xvis[:, 0].min() - 1, Xvis[:, 0].max() + 1
y_min, y_max = Xvis[:, 1].min() - 1, Xvis[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
# create the title that will be shown on the plot
titles = ['Linear kernel','RBF kernel','Polynomial kernel','Sigmoid kernel']


for i, clf in enumerate((l, r, p, s)):
    #defines how many plots: 2 rows, 2columns=> leading to 4 plots
    plt.subplot(2, 2, i + 1) #i+1 is the index
    #space between plots
    plt.subplots_adjust(wspace=0.4, hspace=0.4)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)
    # Plot also the training points
    plt.scatter(Xvis[:, 0], Xvis[:, 1], c=yvis, cmap=plt.cm.PuBuGn, edgecolors='grey')
    plt.xlabel('MEAN_RR')
    plt.ylabel('SDRR')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    # plt.xticks(())
    # plt.yticks(())
    plt.title(titles[i])
    plt.show()

"""Linear Kernel"""

linearSVM = SVC(kernel='linear', C=1, decision_function_shape='ovr')

linear = linearSVM.fit(X_train[:100000], y_train[:100000])

y_pred = linear.predict(X_test)

print('\n------------------ Confusion Matrix -----------------\n')
print(confusion_matrix(y_test, y_pred))

print('\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))
print('Balanced Accuracy: {:.2f}\n'.format(balanced_accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

print('\n--------------- Classification Report ---------------\n')
print(classification_report(y_test, y_pred))

arr = [[59901,1016,5755],
 [17131,3078,1136],
 [24879,1499,8702]]

plt.figure(figsize = (10,7))
sns.set(font_scale=2)
sns.heatmap(arr/np.sum(arr), annot=True, fmt='.2%')

"""RBF Kernel"""

rbf = SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovr').fit(X_train[:100000], y_train[:100000])

y_pred = rbf.predict(X_test)

print('\n------------------ Confusion Matrix -----------------\n')
print(confusion_matrix(y_test, y_pred))

print('\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))
print('Balanced Accuracy: {:.2f}\n'.format(balanced_accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

print('\n--------------- Classification Report ---------------\n')
print(classification_report(y_test, y_pred))

arr = [[62226,860,3586],
 [ 8776,11115,1454],
 [12104,1028,21948]]

plt.figure(figsize = (10,7))
sns.set(font_scale=2)
sns.heatmap(arr/np.sum(arr), annot=True, fmt='.2%')

"""Polynomial Kernel"""

poly = SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovr').fit(X_train[:100000], y_train[:100000])

y_pred = poly.predict(X_test)

print('\n------------------ Confusion Matrix -----------------\n')
print(confusion_matrix(y_test, y_pred))

print('\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))
print('Balanced Accuracy: {:.2f}\n'.format(balanced_accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

print('\n--------------- Classification Report ---------------\n')
print(classification_report(y_test, y_pred))

arr = [[64135,734,1803],
 [12933,7991,421],
 [21779,1398,11903]]

plt.figure(figsize = (10,7))
sns.set(font_scale=2)
sns.heatmap(arr/np.sum(arr), annot=True, fmt='.2%')

"""Sigmoid Kernel"""

sig = SVC(kernel='sigmoid', C=1, decision_function_shape='ovr').fit(X_train[:100000], y_train[:100000])

y_pred = sig.predict(X_test)

print('\n------------------ Confusion Matrix -----------------\n')
print(confusion_matrix(y_test, y_pred))

print('\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))
print('Balanced Accuracy: {:.2f}\n'.format(balanced_accuracy_score(y_test, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))

print('\n--------------- Classification Report ---------------\n')
print(classification_report(y_test, y_pred))

arr = [[49296,15177,2199],
 [16979,2674,1692],
 [20840,8780,5460]]

plt.figure(figsize = (10,7))
sns.set(font_scale=2)
sns.heatmap(arr/np.sum(arr), annot=True, fmt='.2%')

from sklearn.decomposition import PCA
import matplotlib.pyplot as pl

pca = PCA(n_components=2, whiten=True).fit(X[:100000])
X_pca = pca.transform(X[:100000])
print('explained variance ratio:', pca.explained_variance_ratio_)
print('Preserved Variance:', sum(pca.explained_variance_ratio_))

# Print scatter plot to view classification of the simplified dataset
target_names = ['no stress', 'time pressure', 'interruption']

pl.figure()

target_list = np.array(y[:100000]).flatten()
for i,t_name in enumerate(target_names):
  pl.scatter(X_pca[target_list == i, 0], X_pca[target_list == i, 1], label=t_name)

pl.legend()
pl.show()

arr = [[5531,5383,868],
 [1209,19464,1485],
 [701,3111,3281]]

plt.figure(figsize = (10,7))
sns.set(font_scale=2)
sns.heatmap(arr/np.sum(arr), annot=True, fmt='.2%')